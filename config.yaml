trainer:
    # see https://lightning.ai/docs/pytorch/stable/common/trainer.html

    #config devices
    accelerator : 'gpu' # or'gpu'
    precision : "16-mixed" # half
    devices : -1
    strategy: "ddp_find_unused_parameters_true"

    # for debugging
    fast_dev_run : False #runs only 1 training and 1 validation batch and the program ends
    limit_train_batches: .1 #1.0 for all batch

    log_every_n_steps: 50
    max_steps : 10000 # or set max_epochs
    accumulate_grad_batches : 2
    val_check_interval: 1000
    #total_batch = batch * accumulate_grad_batches * num_batch

    #default
    benchmark: True #set true if input size is not changing
    enable_checkpointing: True #saves the most recent model to a single checkpoint after each epoch
    logger: True #ses the default TensorBoardLogger 

dataloader:
    batch_size : 64 # bs per gpu
    shuffle : True
    num_workers : 0 #0 for debug
    pin_memory : True
    drop_last : False

ema:
    decay: .995

optimizer:
    lr: 1.0e-5
    betas: [.9,.999]
    weight_decay: .001

scheduler:
    warm_up_epochs: 1000
    last_lr: 1.0e-6
    #cosine 

seed: 42
